{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea9092a",
   "metadata": {},
   "source": [
    "## Important parameters\n",
    "<br>xt_ocean: longitude, length 3600\n",
    "<br>yt_ocean: latitude, length 1500\n",
    "<br> [mind map](https://miro.com/app/board/o9J_lM4N1Pg=/?fromRedirect=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1795d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4\n",
    "import keras\n",
    "import xarray,numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, add\n",
    "from keras.layers.core import  Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e6d28",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6208fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path =['C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_01.nc']\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t = ds.variables['eta_t'][:]\n",
    "eta_t_numpy = np.array(eta_t)\n",
    "eta_t_numpy = np.nan_to_num(eta_t_numpy)\n",
    "eta_t_numpy.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cc7a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = eta_t_numpy[0,:,:]\n",
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbbbf87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda3780",
   "metadata": {},
   "source": [
    "# fixed missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(input_data)):\n",
    "    arr = input_data[i]\n",
    "    arr[arr == -32768] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83b7a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a979be",
   "metadata": {},
   "source": [
    "# Normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67afc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    norm = np.linalg.norm(matrix)\n",
    "    print(norm)\n",
    "    matrix = matrix/norm  # normalized matrix\n",
    "    return matrix\n",
    "\n",
    "normalize_matrix(input_data)\n",
    "input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31fe930",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data[-40]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cee3e3",
   "metadata": {},
   "source": [
    "# Split array into 128*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c522bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "lat = 128\n",
    "long = 128\n",
    "\n",
    "pad_rows = math.ceil(3600/lat)*lat - 3600\n",
    "pad_cols = math.ceil(1500/long)*long - 1500\n",
    "print(pad_rows)\n",
    "print(pad_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b839a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data=np.pad(input_data,((0,pad_cols),(0,pad_rows)), 'constant',constant_values=(0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86dbb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8538d5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = np.array_split(input_data,len(input_data)/lat,axis=0)\n",
    "input_data_split = []\n",
    "for i in range(len(l)):\n",
    "    dd = np.array_split(l[i],29,axis=1)\n",
    "    input_data_split += dd\n",
    "input_data_split = np.array(input_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e09db16",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_split.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c60c7",
   "metadata": {},
   "source": [
    "# split train set & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(input_data_split, test_size=0.33)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6fd39",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e31571",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = keras.Input(shape=(128, 128,1))\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu',strides=(1,1))(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same',strides=(2,2))(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((4, 4))(x)\n",
    "decoded = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f45beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(train_set, train_set,\n",
    "                epochs=100, validation_data=(test_set, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515131c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb24b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
