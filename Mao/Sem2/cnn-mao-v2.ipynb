{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1795d601",
   "metadata": {},
   "outputs": [],
   "source": [
    "import netCDF4 as nc4\n",
    "import keras\n",
    "import xarray,numpy as np\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, add\n",
    "from keras.layers.core import  Dense, Dropout, Activation, Flatten, Reshape\n",
    "from keras import regularizers\n",
    "from keras.regularizers import l2\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, UpSampling2D, ZeroPadding2D\n",
    "from keras.utils import np_utils\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b71e6d28",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6208fbe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 1500, 3600)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path =['C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_01.nc']\n",
    "file_path.append('C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_02.nc')\n",
    "# file_path.append('C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_03.nc')\n",
    "# file_path.append('C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_04.nc')\n",
    "# file_path.append('C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_05.nc')\n",
    "# file_path.append('C:/Users/myxll/OneDrive - The University of Melbourne/capstone/data/OFAM_2017/ocean_eta_t/ocean_eta_t_2000_06.nc')\n",
    "ds = nc4.MFDataset(file_path)\n",
    "eta_t = ds.variables['eta_t'][:]\n",
    "eta_t_numpy = np.array(eta_t)\n",
    "eta_t_numpy = np.nan_to_num(eta_t_numpy)\n",
    "eta_t_numpy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeda3780",
   "metadata": {},
   "source": [
    "# fixed missing values with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1d6777e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fix_missing_value(input_data):\n",
    "    for i in range(0,len(input_data)):\n",
    "        arr = input_data[i]\n",
    "        arr[arr == -32768] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a979be",
   "metadata": {},
   "source": [
    "# Normalize the input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d67afc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_matrix(matrix):\n",
    "    norm = np.linalg.norm(matrix)\n",
    "    matrix = matrix/norm  # normalized matrix\n",
    "    return matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2cee3e3",
   "metadata": {},
   "source": [
    "# Split array into 128*128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef551a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "lat = 256\n",
    "long = 256\n",
    "def split_array(input_data):\n",
    "    pad_rows = math.ceil(3600/lat)*lat - 3600\n",
    "    pad_cols = math.ceil(1500/long)*long - 1500\n",
    "#     print(pad_rows)\n",
    "#     print(pad_cols)\n",
    "    \n",
    "    input_data=np.pad(input_data,((0,pad_cols),(0,pad_rows)), 'constant',constant_values=(0,0)) # padding with zeros\n",
    "    \n",
    "    l = np.array_split(input_data,len(input_data)/lat,axis=0)\n",
    "    input_data_split = []\n",
    "    for i in range(len(l)):\n",
    "        dd = np.array_split(l[i],29,axis=1)\n",
    "        input_data_split += dd\n",
    "    input_data_split = np.array(input_data_split)\n",
    "    return input_data_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc7a165",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n",
      "(1500, 3600)\n"
     ]
    }
   ],
   "source": [
    "input_data_split = []\n",
    "lat = 256\n",
    "long = 256\n",
    "for i in range(len(eta_t_numpy)):\n",
    "    input_data = eta_t_numpy[i,:,:]\n",
    "    print(np.array(input_data).shape)\n",
    "    fix_missing_value(input_data)\n",
    "\n",
    "    normalize_matrix(input_data)\n",
    "\n",
    "#     input_data_split.append(split_array(input_data)[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data_split= np.array(input_data_split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c96060",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('128x128_6months',input_data_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203c60c7",
   "metadata": {},
   "source": [
    "# split train set & validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77541d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(input_data_split, test_size=0.33)\n",
    "train_set.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6fd39",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e31571",
   "metadata": {},
   "outputs": [],
   "source": [
    "lat = 1024\n",
    "long = 1024\n",
    "\n",
    "input_img = keras.Input(shape=(lat, long,1))\n",
    "\n",
    "x = layers.Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(8, (3, 3), activation='relu',strides=(1,1))(x)\n",
    "x = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = layers.Conv2D(4, (3, 3), activation='relu', padding='same',strides=(2,2))(x)\n",
    "encoded = layers.MaxPooling2D((2, 2), padding='same')(x)\n",
    "\n",
    "x = layers.Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2DTranspose(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((2, 2))(x)\n",
    "x = layers.Conv2DTranspose(16, (3, 3), activation='relu', padding='same')(x)\n",
    "x = layers.UpSampling2D((4, 4))(x)\n",
    "decoded = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = keras.Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f45beb",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = autoencoder.fit(train_set, train_set,\n",
    "                epochs=100, validation_data=(test_set, test_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd1eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'], loc='upper right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb24b41",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
